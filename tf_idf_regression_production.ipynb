{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf-idf-regression-production",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vlZP7WSLvQP8ElH3tXQjmY1_f89dawY3",
      "authorship_tag": "ABX9TyP0YC6mQ/mMHhoDUprCPSCs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaruchG/tfidf-regression-experiments/blob/main/tf_idf_regression_production.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWMddaXp7Thk"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import math\n",
        "from scipy.sparse import vstack\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from sklearn.preprocessing._data import normalize\n",
        "from sklearn.utils.validation import _deprecate_positional_args, check_array, FLOAT_DTYPES, check_is_fitted\n",
        "from sklearn.utils.fixes import _astype_copy_false\n",
        "import scipy.sparse as sp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MoVGXJV7c9s"
      },
      "source": [
        "def _document_frequency(X):\n",
        "    \"\"\"Count the number of non-zero values for each feature in sparse X.\"\"\"\n",
        "    if sp.isspmatrix_csr(X):\n",
        "        return np.bincount(X.indices, minlength=X.shape[1])\n",
        "    else:\n",
        "        return np.diff(X.indptr)\n",
        "      \n",
        "class TfidfTransformerCustom(TransformerMixin, BaseEstimator):\n",
        "\n",
        "    @_deprecate_positional_args\n",
        "    def __init__(self, *, norm='l2', use_idf=True, smooth_idf=True,\n",
        "                 sublinear_tf=False):\n",
        "        self.norm = norm\n",
        "        self.use_idf = use_idf\n",
        "        self.smooth_idf = smooth_idf\n",
        "        self.sublinear_tf = sublinear_tf\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = check_array(X, accept_sparse=('csr', 'csc'))\n",
        "        if not sp.issparse(X):\n",
        "            X = sp.csr_matrix(X)\n",
        "        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64\n",
        "\n",
        "        if self.use_idf:\n",
        "            n_samples, n_features = X.shape\n",
        "            df = _document_frequency(X)\n",
        "            df = df.astype(dtype, **_astype_copy_false(df))\n",
        "\n",
        "            # perform idf smoothing if required\n",
        "            df += int(self.smooth_idf)\n",
        "            n_samples += int(self.smooth_idf)\n",
        "\n",
        "            # log+1 instead of log makes sure terms with zero idf don't get\n",
        "            # suppressed entirely.\n",
        "            # print(n_samples)\n",
        "            idf = np.log(n_samples / df) + 1\n",
        "            # print(df)\n",
        "            # print(\"idf\", idf)\n",
        "            self._idf_diag = sp.diags(idf, offsets=0,\n",
        "                                      shape=(n_features, n_features),\n",
        "                                      format='csr',\n",
        "                                      dtype=dtype)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, copy=True):\n",
        "        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)\n",
        "        if not sp.issparse(X):\n",
        "            X = sp.csr_matrix(X, dtype=np.float64)\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        if self.sublinear_tf:\n",
        "            np.log(X.data, X.data)\n",
        "            X.data += 1\n",
        "\n",
        "        if self.use_idf:\n",
        "            # idf_ being a property, the automatic attributes detection\n",
        "            # does not work as usual and we need to specify the attribute\n",
        "            # name:\n",
        "            check_is_fitted(self, attributes=[\"idf_\"],\n",
        "                            msg='idf vector is not fitted')\n",
        "\n",
        "            expected_n_features = self._idf_diag.shape[0]\n",
        "            if n_features != expected_n_features:\n",
        "                raise ValueError(\"Input has n_features=%d while the model\"\n",
        "                                 \" has been trained with n_features=%d\" % (\n",
        "                                     n_features, expected_n_features))\n",
        "            # *= doesn't work\n",
        "            # print(\"X\", X.toarray())\n",
        "            # tf * idf but tf is not yet normalized\n",
        "            X = X * self._idf_diag\n",
        "            # print(\"post\", X.toarray())\n",
        "            # print(\"Sum\", sum(X.toarray()[3]))\n",
        "\n",
        "        if self.norm:\n",
        "            # normalizes it but in different order than tutorial so it's normalized post tf-idf computation without normalizing the tf, default is l2 if want length use l1\n",
        "            X = normalize(X, norm=self.norm, copy=False)\n",
        "            # print(\"post\", X[3, 0])\n",
        "        # X = X * self._idf_diag\n",
        "        return X\n",
        "\n",
        "    @property\n",
        "    def idf_(self):\n",
        "        # if _idf_diag is not set, this will raise an attribute error,\n",
        "        # which means hasattr(self, \"idf_\") is False\n",
        "        return np.ravel(self._idf_diag.sum(axis=0))\n",
        "\n",
        "    @idf_.setter\n",
        "    def idf_(self, value):\n",
        "        value = np.asarray(value, dtype=np.float64)\n",
        "        n_features = value.shape[0]\n",
        "        self._idf_diag = sp.spdiags(value, diags=0, m=n_features,\n",
        "                                    n=n_features, format='csr')\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {'X_types': 'sparse'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEMxS_kJ7mzd"
      },
      "source": [
        "imdb = pd.read_csv(\"/content/drive/My Drive/datasets/IMDB_Dataset.csv\")\n",
        "trainX = imdb['review'][:4000]\n",
        "trainY = imdb['sentiment'][:4000]\n",
        "validX = imdb['review'][4001:]\n",
        "validY = imdb['sentiment'][4001:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOP27kbz7std"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
        "vectorized = vectorizer.fit_transform(trainX)\n",
        "tfidfModel = TfidfTransformerCustom(use_idf=True, smooth_idf=False)\n",
        "t = tfidfModel.fit(vectorized)\n",
        "tout = t.transform(vectorized)\n",
        "arr = tout.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-MaCpiC79qz"
      },
      "source": [
        "clf = LogisticRegression(random_state=0).fit(arr, trainY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqoi1DqT7_Bi"
      },
      "source": [
        "validVectorized = vectorizer.transform(validX)\n",
        "validtout = t.transform(validVectorized)\n",
        "arrValid = validtout.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CRN4Guu8Feo",
        "outputId": "ea1eee44-3901-4459-fe6e-0e0e67c492eb"
      },
      "source": [
        "#Validation metrics\n",
        "preds = clf.predict(arrValid)\n",
        "print(classification_report(validY, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.85      0.85     22973\n",
            "    positive       0.85      0.86      0.86     23026\n",
            "\n",
            "    accuracy                           0.86     45999\n",
            "   macro avg       0.86      0.86      0.86     45999\n",
            "weighted avg       0.86      0.86      0.86     45999\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pz4eGgv8LNC",
        "outputId": "372716b2-80fe-4cf7-8814-61b1a68945f1"
      },
      "source": [
        "#Training metrics\n",
        "preds = clf.predict(arr)\n",
        "print(classification_report(trainY, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.94      0.94      0.94      2027\n",
            "    positive       0.94      0.94      0.94      1973\n",
            "\n",
            "    accuracy                           0.94      4000\n",
            "   macro avg       0.94      0.94      0.94      4000\n",
            "weighted avg       0.94      0.94      0.94      4000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF7AP8258Xs6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}